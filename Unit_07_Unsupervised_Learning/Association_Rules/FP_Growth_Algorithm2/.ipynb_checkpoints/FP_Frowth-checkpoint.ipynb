{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "import multiprocessing\n",
    "import unittest\n",
    "\n",
    "import mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, item, count=0):\n",
    "        self.item = item\n",
    "        self.count = count\n",
    "        self.parent = None\n",
    "        self.children = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.item is not None:\n",
    "            s = 'item: {} count: {}  '.format(self.item, self.count)\n",
    "        else:\n",
    "            s = 'root \\n'\n",
    "        s += 'children: '\n",
    "        for child in self.children:\n",
    "            s += str(child) + ' '\n",
    "\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPTree():\n",
    "    def __init__(self):\n",
    "        # dict[item] = [Node1, Node2...]\n",
    "        self.header_table = {}\n",
    "        self.item_counter = {}\n",
    "        self.root = Node(None)\n",
    "\n",
    "    def add_tran(self, tran, weight=1):\n",
    "        ptr = self.root\n",
    "        for item in tran:\n",
    "            if item in ptr.children:\n",
    "                ptr.children[item].count += weight\n",
    "                self.item_counter[item] += weight\n",
    "                ptr = ptr.children[item]\n",
    "            else:\n",
    "                new_node = Node(item, weight)\n",
    "                new_node.parent = ptr\n",
    "                ptr.children[item] = new_node\n",
    "                if item in self.header_table:\n",
    "                    self.header_table[item].append(new_node)\n",
    "                    self.item_counter[item] += weight\n",
    "                else:\n",
    "                    self.header_table[item] = [new_node]\n",
    "                    self.item_counter[item] = weight\n",
    "                ptr = new_node\n",
    "\n",
    "        return\n",
    "\n",
    "    def mine(self, min_cnt=1):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            [list of frequent patterns, list of fp count]\n",
    "        \"\"\"\n",
    "        fp, fp_count = [], []\n",
    "        for item in self.header_table:\n",
    "            if self.item_counter[item] >= min_cnt:\n",
    "                fp.append([item])\n",
    "                fp_count.append(self.item_counter[item])\n",
    "\n",
    "                cond_trans, weights = self.get_conditional_tran(item, min_cnt)\n",
    "                cond_tree = FPTree()\n",
    "                for tran, weight in zip(cond_trans, weights):\n",
    "                    assert item not in tran, (item, tran, weight)\n",
    "                    cond_tree.add_tran(tran, weight)\n",
    "                cond_fp, cond_fp_count = cond_tree.mine(min_cnt)\n",
    "                if cond_fp:\n",
    "                    cond_fp = [i + [item] for i in cond_fp]\n",
    "                    fp += cond_fp\n",
    "                    fp_count += cond_fp_count\n",
    "\n",
    "        assert len(fp) == len(fp_count)\n",
    "        if fp:\n",
    "            fp = [sorted(i) for i in fp]\n",
    "            tmp = list(zip(fp, fp_count))\n",
    "            tmp = sorted(tmp, key=lambda x: (len(x[0]), x[0]))\n",
    "            fp, fp_count = list(zip(*tmp))\n",
    "\n",
    "        return fp, fp_count\n",
    "\n",
    "    def get_conditional_tran(self, item, min_cnt=1):\n",
    "        \"\"\"\n",
    "        excluding item\n",
    "        return:\n",
    "        [list of items, list of weight]\n",
    "        \"\"\"\n",
    "        trans, weights = [], []\n",
    "        for node in self.header_table[item]:\n",
    "            # if node.count >= min_cnt:\n",
    "            tmp_tran = []\n",
    "            ptr = node.parent\n",
    "            # while ptr.item: // wrong when item == 0\n",
    "            while ptr.item != None:\n",
    "                tmp_tran.append(ptr.item)\n",
    "                ptr = ptr.parent\n",
    "            if tmp_tran:\n",
    "                trans.append(tmp_tran)\n",
    "                weights.append(node.count)\n",
    "\n",
    "        return trans, weights\n",
    "\n",
    "    def print_tree(self):\n",
    "        l = [self.root]\n",
    "        while l:\n",
    "            next_l = []\n",
    "            for node in l:\n",
    "                print(node)\n",
    "                next_l += node.children.values()\n",
    "            l = next_l\n",
    "            print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MineByItem(multiprocessing.Process):\n",
    "    def __init__(self, tree, min_cnt, fp_list, queue):\n",
    "        multiprocessing.Process.__init__(self)\n",
    "        self.tree = tree\n",
    "        self.min_cnt = min_cnt\n",
    "        self.queue = queue\n",
    "        self.fp_list = fp_list\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                break\n",
    "            else:\n",
    "                item = self.queue.get()\n",
    "                self.fp_list.append(([item], self.tree.item_counter[item]))\n",
    "                cond_trans, weights = self.tree.get_conditional_tran(item, self.min_cnt)\n",
    "                cond_tree = FPTree()\n",
    "                for tran, weight in zip(cond_trans, weights):\n",
    "                    assert item not in tran, (item, tran, weight)\n",
    "                    cond_tree.add_tran(tran, weight)\n",
    "                cond_fp, cond_fp_count = cond_tree.mine(self.min_cnt)\n",
    "                if cond_fp:\n",
    "                    cond_fp = [i + [item] for i in cond_fp]\n",
    "                    cond_fp = [sorted(fp) for fp in cond_fp]\n",
    "                    self.fp_list += list(zip(cond_fp, cond_fp_count))\n",
    "\n",
    "def parallel_mine(tree, min_cnt=1, n_jobs=4):\n",
    "    mgr = multiprocessing.Manager()\n",
    "    fp_list = mgr.list()\n",
    "    queue = multiprocessing.Queue()\n",
    "\n",
    "    for item in tree.header_table:\n",
    "        if tree.item_counter[item] >= min_cnt:\n",
    "            queue.put(item)\n",
    "\n",
    "    process_list = [MineByItem(tree, min_cnt, fp_list, queue) for _ in range(n_jobs)]\n",
    "    for process in process_list: process.start()\n",
    "    for process in process_list: process.join()\n",
    "    fp_list = list(fp_list)\n",
    "    if fp_list:\n",
    "        fp_list = sorted(fp_list, key=lambda x: (len(x[0]), x[0]))\n",
    "\n",
    "    return fp_list\n",
    "\n",
    "\n",
    "class CountWorker(multiprocessing.Process):\n",
    "    def __init__(self, count_list, queue):\n",
    "        multiprocessing.Process.__init__(self)\n",
    "        self.count_list = count_list\n",
    "        self.queue = queue\n",
    "\n",
    "    def run(self):\n",
    "        counter = {}\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                break\n",
    "            else:\n",
    "                sub_trans = self.queue.get()\n",
    "                for tran in sub_trans:\n",
    "                    for item in tran:\n",
    "                        counter[item] = counter.get(item, 0) + 1\n",
    "        self.count_list.append(counter)\n",
    "\n",
    "\n",
    "def parallel_count(trans, n_jobs=1):\n",
    "    mgr = multiprocessing.Manager()\n",
    "    count_list = mgr.list()\n",
    "    queue = multiprocessing.Queue()\n",
    "\n",
    "    batch_len = 10000\n",
    "    for idx in range(len(trans) // batch_len + 1):\n",
    "        queue.put(trans[idx * batch_len: (idx + 1) * batch_len])\n",
    "\n",
    "    process_list = [CountWorker(count_list, queue) for _ in range(n_jobs)]\n",
    "    for process in process_list: process.start()\n",
    "    for process in process_list: process.join()\n",
    "\n",
    "    counter = {}\n",
    "    for sub_counter in count_list:\n",
    "        for item in sub_counter:\n",
    "            counter[item] = counter.get(item, 0) + sub_counter[item]\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def fp_growth(trans, min_support=0.1, use_log=False, n_jobs=1):\n",
    "    \"\"\"FP growth algorithm for frequent patterns mining\n",
    "    Arguments:\n",
    "        trans: a list of transactions, each transaction is a list of items\n",
    "        min_support (float): minimum support, default: 0.1\n",
    "        use_log: logging, default: False\n",
    "        n_jobs (int): when n_jobs > 1, mining the frequent patterns in paralle. default: 1\n",
    "\n",
    "    Return:\n",
    "        list of [pattern, frequency] tuples, pattern is an items list\n",
    "    \"\"\"\n",
    "    if use_log:\n",
    "        logging.basicConfig(filename='fp_tree.log', format='%(asctime)s %(message)s',\n",
    "            level=logging.DEBUG, datefmt='%Y/%m/%d %I:%M:%S %p')\n",
    "\n",
    "    if use_log:\n",
    "        logging.info('Begin to count items')\n",
    "    # count and sort\n",
    "    counter = {}\n",
    "    min_cnt = int(min_support * len(trans))\n",
    "    if min_cnt < 1: return []\n",
    "\n",
    "#     if n_jobs == 1:\n",
    "    if True:\n",
    "        for tran in trans:\n",
    "            for item in tran:\n",
    "                counter[item] = counter.get(item, 0) + 1\n",
    "#     else:\n",
    "#         parallel_count(trans, n_jobs)\n",
    "\n",
    "    if use_log:\n",
    "        logging.info('Counting finished')\n",
    "\n",
    "    frequent_item = [item for item in counter if counter[item] >= min_cnt]\n",
    "\n",
    "    # build FP-tree\n",
    "    fp_tree = FPTree()\n",
    "    if use_log:\n",
    "        logging.info('Begin to add transactions')\n",
    "\n",
    "    # add trans\n",
    "    for tran in trans:\n",
    "        tran = [item for item in tran if item in frequent_item]\n",
    "        tran = list(set(tran))\n",
    "        tran = sorted(tran, key=lambda x: (counter[x], x), reverse=True) # the order is very important\n",
    "        if tran: fp_tree.add_tran(tran)\n",
    "\n",
    "    if use_log:\n",
    "        logging.info('Adding transactions finished')\n",
    "        logging.info('Begin to mine fp')\n",
    "\n",
    "    # mine pattern\n",
    "    if n_jobs == 1:\n",
    "        res = fp_tree.mine(min_cnt)\n",
    "        res = list(zip(*res))\n",
    "    elif n_jobs > 1:\n",
    "        res = parallel_mine(fp_tree, min_cnt, n_jobs)\n",
    "\n",
    "    if use_log:\n",
    "        logging.info('Mining fp finished')\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFPTree(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        pass\n",
    "\n",
    "    def test_add_tran(self):\n",
    "        tree = FPTree()\n",
    "        tree.add_tran([1, 2, 3, 4])\n",
    "        tree.add_tran([1, 2, 3])\n",
    "        tree.add_tran([1, 2])\n",
    "        tree.add_tran([1])\n",
    "\n",
    "        self.assertEqual(len(tree.header_table), 4)\n",
    "        self.assertEqual(len(tree.item_counter), 4)\n",
    "\n",
    "        for i in range(1, 5):\n",
    "            self.assertEqual(tree.item_counter[i] + i, 5)\n",
    "        for i in range(1, 5):\n",
    "            self.assertEqual(len(tree.header_table[i]), 1)\n",
    "\n",
    "        tree.add_tran([1, 3, 4], 2)\n",
    "        self.assertEqual(len(tree.header_table[3]), 2)\n",
    "        self.assertEqual(len(tree.header_table[4]), 2)\n",
    "        self.assertEqual(tree.item_counter[1], 6)\n",
    "        self.assertEqual(tree.item_counter[2], 3)\n",
    "\n",
    "    def test_get_conditional_tran(self):# test get_conditional_tran\n",
    "        tree = FPTree()\n",
    "        tree.add_tran([1, 2, 3, 4])\n",
    "        tree.add_tran([1, 2, 3])\n",
    "        tree.add_tran([1, 2])\n",
    "        tree.add_tran([1])\n",
    "        tree.add_tran([1, 3, 4], 2)\n",
    "\n",
    "        trans, weights = tree.get_conditional_tran(4, 1)\n",
    "        self.assertEqual(len(trans), 2)\n",
    "        self.assertEqual(weights[0], 1)\n",
    "        self.assertEqual(weights[1], 2)\n",
    "\n",
    "    def compare_with_apriori(self, dataset, test_parallel=False):\n",
    "        support = 0.4\n",
    "        # apriori\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(dataset).transform(dataset)\n",
    "        df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        df = apriori(df, min_support=support, use_colnames=True)\n",
    "        frequency = df.iloc[:, 0].to_numpy().tolist()\n",
    "        pattern = df.iloc[:, 1].to_numpy().tolist()\n",
    "        pattern = [sorted(list(i)) for i in pattern]\n",
    "        apriori_result = sorted(zip(pattern, frequency), key=lambda x: (len(x[0]), x[0]))\n",
    "\n",
    "        # fp-growth\n",
    "        if test_parallel:\n",
    "            fp_result = fp_growth(dataset, support, n_jobs=4)\n",
    "        else:\n",
    "            fp_result = fp_growth(dataset, support)\n",
    "\n",
    "        self.assertEqual(len(apriori_result), len(fp_result))\n",
    "        for idx in range(len(apriori_result)):\n",
    "            self.assertEqual(apriori_result[idx][0], fp_result[idx][0])\n",
    "            self.assertTrue(round(apriori_result[idx][1] * len(dataset)) == fp_result[idx][1]) # float point calculation error\n",
    "\n",
    "    def test_testify_apriori_small_dataset(self):\n",
    "        dataset = [['1', '2', '3', '4', '5', '6'],\n",
    "                   ['7', '2', '3', '4', '5', '6'],\n",
    "                   ['1', '11', '4', '5'],\n",
    "                   ['1', '10', '8', '4', '6'],\n",
    "                   ['8', '2', '2', '4', '9', '5']]\n",
    "\n",
    "        self.compare_with_apriori(dataset)\n",
    "        self.compare_with_apriori(dataset, True)\n",
    "\n",
    "\n",
    "    def test_testify_apriori_fake_dataset(self):\n",
    "        # make fake dataset\n",
    "        total_trans = 10000\n",
    "        max_trans_len = 20\n",
    "        dataset = []\n",
    "        for _ in range(total_trans):\n",
    "            tmp = [np.random.randint(max_trans_len) for __ in range(max_trans_len)]\n",
    "            tmp = list(set(tmp))\n",
    "            dataset.append(tmp)\n",
    "\n",
    "#         output_lines = []\n",
    "#         for tran in dataset:\n",
    "#             tran = [str(i) for i in tran]\n",
    "#             tran = '[' + ', '.join(tran) + ']'\n",
    "#             output_lines.append(tran)\n",
    "\n",
    "#         with open('dataset.dat', 'w') as file:\n",
    "#             output_lines = '[' + ',\\n'.join(output_lines) + ']'\n",
    "#             file.write(output_lines)\n",
    "\n",
    "        self.compare_with_apriori(dataset)\n",
    "        self.compare_with_apriori(dataset, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: C:\\Users\\PR269KB\\AppData\\Roaming\\jupyter\\runtime\\kernel-28a11521-9ff6-4aee-b23e-8525d8a070dd (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute 'C:\\Users\\PR269KB\\AppData\\Roaming\\jupyter\\runtime\\kernel-28a11521-9ff6-4aee-b23e-8525d8a070dd'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PR269KB\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8f in position 14: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-68ba300e11fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mone_pass_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# prepare dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8f in position 14: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "running_time = []\n",
    "for total_trans in [i * 5000 for i in range(1, 14)]:\n",
    "    print(total_trans)\n",
    "    min_support = 0.01\n",
    "#     if min_support * len(total_trans) < 100: min_support = 100 / len(total_trans)\n",
    "        \n",
    "    one_pass_time = []\n",
    "    \n",
    "    with open(\"test.txt\", 'r') as file: lines = file.readlines()\n",
    "    \n",
    "    # prepare dataset\n",
    "    sents = []\n",
    "    lines = [line.strip('\\n').strip('0\\t').strip('1\\t') for line in lines]\n",
    "    for line in lines: sents += line.split('\\t')\n",
    "    sents = [sent.split(' ') for sent in sents]   \n",
    "    print(len(lines), len(sents))\n",
    "    \n",
    "#     total_trans= int(total_trans)\n",
    "#     max_trans_len = 50\n",
    "#     items_num = 1000000\n",
    "#     dataset = []\n",
    "#     for _ in range(total_trans):\n",
    "#         tmp = [np.random.randint(items_num) for __ in range(np.random.randint(max_trans_len))]\n",
    "#         tmp = list(set(tmp))\n",
    "#         dataset.append(tmp)\n",
    "        \n",
    "\n",
    "    dataset = sents[:total_trans]\n",
    "\n",
    "    # fp-tree sequential\n",
    "    start = time.time()\n",
    "    pattern = fp_growth(dataset, min_support, True, n_jobs=1)\n",
    "    elapsed = time.time() - start\n",
    "    one_pass_time.append(elapsed)\n",
    "\n",
    "\n",
    "    #fp-tree parallel\n",
    "    start = time.time()\n",
    "    pattern = fp_growth(dataset, min_support, True, n_jobs=4)\n",
    "    elapsed = time.time() - start\n",
    "    one_pass_time.append(elapsed)\n",
    "    \n",
    "    \n",
    "    # apriori \n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(dataset).transform(dataset)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    start = time.time()\n",
    "    fp = apriori(df, min_support=min_support)\n",
    "    elapsed = time.time() - start\n",
    "    one_pass_time.append(elapsed)\n",
    "    \n",
    "    running_time.append(one_pass_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['fp-growth', 'fp-growth-parallel', 'apriori']\n",
    "dataset_size = [i * 5000 for i in range(1, 14)]\n",
    "running_time = np.asarray(running_time)\n",
    "for idx in range(len(names)): plt.plot(dataset_size, running_time[:, idx], label=names[idx])\n",
    "plt.legend()\n",
    "plt.xlabel('dataset size')\n",
    "plt.ylabel('time (s)')\n",
    "plt.savefig('run_time.png', dpi=300)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
